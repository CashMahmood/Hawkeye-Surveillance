================================================================================
          PROJECT HAWKEYE: AI-POWERED TACTICAL SURVEILLANCE SYSTEM
          COMPREHENSIVE PROJECT PRESENTATION & DEFENSE DOCUMENT
================================================================================
Role: Professor, Department of Computer Science & Engineering (CSE)
Target Audience: Academic Evaluation Board / Technical Examiners
Status: Final Year / Research-Level Project Defense
Date: January 21, 2026

1. PROBLEM STATEMENT AND MOTIVATION
-----------------------------------
Traditional surveillance systems rely heavily on manual human monitoring, which is prone to fatigue, distraction, and delayed response times. In high-stakes environments—such as tactical zones, restricted facilities, or public squares—the ability to detect threats (firearms, knives, or suspicious groupings) in real-time is critical.

Motivation: 
- To bridge the gap between low-cost IoT hardware and high-performance Deep Learning.
- To create a "Tactical HUD" that provides immediate situational awareness.
- To solve the "False Positive" problem common in vision systems where static objects (e.g., furniture) are misidentified as weapons.

2. OVERALL WORKING PRINCIPLE
----------------------------
Project Hawkeye operates on a "Hybrid Edge-Cloud" paradigm. An ESP32-S3 microcontroller handles image acquisition and wirelessly streams MJPEG data to a centralized "Inference Engine" (FastAPI Backend). 

The system functions end-to-end as follows:
- Acquisition: The camera captures a raw visual feed.
- Transport: Data is transmitted over a low-latency Wi-Fi uplink.
- Intelligence: The backend runs a dual-model neural pipeline to detect humans and specific weapon classes.
- Logic: Spatial and temporal filters validate detections to prevent false alarms.
- Alerting: Validated threats are logged, archived with snapshots, and broadcasted to a React-based Tactical HUD for human intervention.

3. TECHNOLOGIES, FRAMEWORKS, AND LANGUAGES
------------------------------------------
- Languages: Python 3.10+ (Backend Logic), JavaScript/TypeScript (Frontend HUD), C++ (ESP32-S3 Firmware).
- AI Frameworks: Ultralytics YOLOv8 (State-of-the-art Real-time Object Detection).
- Backend: FastAPI (Asynchronous high-concurrency framework for low-latency ingest).
- Frontend: React.js + Tailwind CSS + Framer Motion (Real-time data visualization and UI animations).
- Database: SQLite (Relational storage for event logs and metadata).
- Computer Vision: OpenCV (Image preprocessing and Bounding Box scaling).

4. MACHINE LEARNING / DEEP LEARNING MODELS
------------------------------------------
Hawkeye employs a "Dual-Head Convergence" architecture:

A. General Intelligence Model (GEN): 
- Model: YOLOv8n (Nano)
- Purpose: Optimized for high-speed detection of "Person" classes.
- Reason: The Nano variant provides the lowest latency on CPU-based servers while maintaining sufficient accuracy for human silhouette detection.

B. Specialized Threat Model (SPEC):
- Model: Custom-trained YOLOv8 head (subh775_threat)
- Classes: Guns (Pistols/Rifles), Knives, Grenades, Explosions.
- Architecture: YOLOv8 CSP (Cross Stage Partial) backbone with a specialized neck for small-object detection.
- Training Approach: Transfer learning on a curated dataset of 5,000+ weapon images under varying lighting and occlusion.
- Selection Reasoning: YOLOv8 was chosen over Faster R-CNN or SSD for its superior speed-to-accuracy ratio in real-time streaming contexts.

5. CAMERA CONFIGURATION
-----------------------
- Type: ESP32-S3 WROOM CAM (AI-enabled SoC).
- Resolution: QVGA (320x240) optimized for 15-20 FPS throughput.
- Frame Rate: Restricted to 15 FPS to balance Wi-Fi bandwidth and CPU inference load.
- Placement: Recommended high-angle (Bird's-eye) or tactical mounting for maximum perimeter coverage.
- Calibration: Auto-Exposure Control (AEC) and Auto-Gain Control (AGC) are enabled via the ESP32 sensor driver to handle varying light intensities.
- Lighting: Hardware is optimized for daylight and well-lit indoor environments. Low-light performance is aided by digital gain but limited by the sensor's small aperture.

6. ALTERNATIVE CAMERA OPTIONS
-----------------------------
| Option | Specs | Advantage | Disadvantage | Approx. Price |
| :--- | :--- | :--- | :--- | :--- |
| **ESP32-CAM (OV2640)** | 2MP, low RAM | Extremely cheap | Narrow BW, no AI vector inst. | $8 - $10 |
| **XIAO ESP32S3 Sense** | 240MHz, 8MB PSRAM | Ultra-compact, fast AI | Requires external antenna | $14 |
| **Raspberry Pi AI Cam** | 12MP, IMX500 NPU | On-device inference (30fps) | High power consumption | $70 |
| **NVIDIA Jetson Nano** | GPU-accelerated | Desktop-grade accuracy | Expensive, bulky | $150+ |

7. SYSTEM PIPELINE (STEP-BY-STEP)
---------------------------------
1. Ingest: OpenCV-based multi-threaded fetcher grabs MJPEG chunks.
2. Pre-processing: Frames are resized to 640px (High Definition) for the weapon head and 320px for the person head.
3. Inference: Parallel execution of YOLOv8 models.
4. Scale-Back: Bounding boxes are scaled from inference resolution back to HUD display dimensions.
5. Spatial Filtering (IoU): Rejection of overlapping redundant detections.
6. Temporal Voting: Detections are stored in a 5-frame buffer; a "Majority Vote" is required for a formal alert.
7. Armed-Contact Check: Identify if weapon BBox is within 100px of a Person BBox (Person-with-Weapon classification).
8. Dispatch: Broadcast JSON metadata via WebSockets to the React HUD.
9. Archival: High-confidence (0.50+) frames are saved as JPEGs; event metadata is written to SQLite.

8. IMPORTANT PARAMETERS & HYPERPARAMETERS
-----------------------------------------
- CONF_THRESH_PERSON: 0.40 (Balance between recall and precision).
- CONF_THRESH_WEAPON: 0.45 (Primary trigger sensitivity).
- CONF_THRESH_ARCHIVE: 0.50 (Strict gate for database persistence).
- STATIC_SUPPRESSION_FRAMES: 30 (Number of frames a weapon must move to not be considered a "closet" or "table").
- WEAPON_PERSISTENCE_CYCLES: 5 frames (The "memory" of the system).
- ASSOCIATION_MARGIN_PX: 100px (Proximity window for armed contact).

9. THRESHOLD LOGIC & PERFORMANCE IMPACT
---------------------------------------
Thresholds were determined through empirical testing:
- Low Threshold (0.30): Increases detections but leads to high false positives (e.g., cell phones seen as knives).
- High Threshold (0.80): Eliminates false positives but may miss weapons held at obscure angles or partially occluded.
- Hawkeye Choice (0.45-0.50): Provides a "Tactical Middle-ground" where temporal voting compensates for lower per-frame confidence.

10. PERFORMANCE & REAL-TIME CONSTRAINTS
---------------------------------------
- Input/Output Dimensions: 320x240 (Stream) -> 640x640 (Inference) -> 320x240 (HUD).
- Processing Speed: ~120-180ms per frame (on modern CPU), allowing for ~5-8 FPS processed throughput.
- Constraints: Bottleneck is typically the CPU's ability to run dual-model inference. Multi-threading ensures the UI remains responsive at 60 FPS even if the inference logic is slightly slower.

11. LIMITATIONS, CHALLENGES, AND ASSUMPTIONS
--------------------------------------------
- Hardware: ESP32-S3 resolution is low; fine details (e.g., a small folding knife) may be lost at distances >10 meters.
- Environment: Extreme "Visual Noise" (e.g., a room full of complex patterns) can lower confidence scores.
- Assumptions: The system assumes a stable 2.4GHz Wi-Fi connection for the stream.

12. FAILURE CASES & EDGE CONDITIONS
-----------------------------------
- Failure Case 1: Motion Blur. Fast-moving weapons may fail to trigger due to pixel smearing.
- Failure Case 2: Occlusion. A gun hidden behind a person's back is invisible to 2D vision.
- Edge Condition Management: The system uses "Persistence Momentum." If a weapon is detected and then vanishes due to blur, the system keeps the BBox alive for 4 cycles to maintain situational awareness.

13. FUTURE IMPROVEMENTS
-----------------------
- Depth Integration: Using LiDAR or Stereo cameras to calculate absolute distance to threat.
- Model Quantization: Using OpenVINO or TensorRT to reach 30+ FPS on consumer hardware.
- Audio Analysis: Integrating gunshot detection (Acoustic Sensors) to complement visual data.

--------------------------------------------------------------------------------
14. ANTICIPATED Q&A (BOARD DEFENSE)
--------------------------------------------------------------------------------

Q1: Why use two YOLO models instead of one?
A: Using one model for everything requires a massive dataset and often compromises accuracy. By decoupling "Person" (general) and "Weapon" (specialized), we can optimize resolution (640px for weapons, 320px for persons) to catch small threat details without slowing down the entire pipeline.

Q2: How do you handle network latency in a tactical scenario?
A: We use an asynchronous MJPEG buffer. The backend never waits for a frame to process before fetching the next one. This ensures we are always seeing the "most recent" frame, even if we have to skip frames to keep up with real-time.

Q3: What happens if the Wi-Fi signal is jammed or lost?
A: The React HUD monitors the "Heartbeat" of the stream. If the frame delta exceeds 1 second, the HUD transitions to "OFFLINE // LINK LOST" status and triggers a visual alert to the operator.

Q4: Is the system ethical regarding privacy?
A: Hawkeye is designed for tactical threat detection, not facial recognition. It identifies "Human Silhouettes" as a class, not individual identities, ensuring compliance with general data protection standards in a security context.
================================================================================
END OF DOCUMENT
================================================================================
